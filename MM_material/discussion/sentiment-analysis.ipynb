{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "## Group Names and Roles\n",
    "\n",
    "- Partner 1 (Role)\n",
    "- Partner 2 (Role)\n",
    "- Partner 3 (Role)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this problem, you will study the sentiment in a data set of tweets collected during the COVID-19 pandemic. To load the data set, run this block: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def grab_tweets():\n",
    "    \"\"\"\n",
    "    The user supplied these data already split into training and test sets. \n",
    "    This function downloads and combines them, returning a single data frame.\n",
    "    No arguments. \n",
    "    \"\"\"\n",
    "\n",
    "    url1 = \"https://raw.githubusercontent.com/PhilChodrow/PIC16A/master/datasets/Corona_NLP_train.csv\"\n",
    "    url2 = \"https://raw.githubusercontent.com/PhilChodrow/PIC16A/master/datasets/Corona_NLP_test.csv\"\n",
    "    \n",
    "    df1 = pd.read_csv(url1, encoding='iso-8859-1')\n",
    "    df2 = pd.read_csv(url2, encoding='iso-8859-1')\n",
    "    \n",
    "    return pd.concat((df1, df2), axis = 0).reset_index().drop(\"index\", axis = 1)\n",
    "\n",
    "df = grab_tweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §1.Take a Look \n",
    "\n",
    "Briefly inspect the data `df`. The `OriginalTweet` column contains the text of the tweet, and the `Sentiment` column contains a text label describing the sentiment of the tweet as described by a crowdsourced worker. \n",
    "\n",
    "Create a `pandas` summary table showing how many tweets there are for each sentiment. \n",
    "\n",
    "**Hint**: `groupby().size()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §2. Create the Term-Document Matrix\n",
    "\n",
    "Now, use the `CountVectorizer` from `sklearn.feature_extraction.text` to construct a term-document matrix. You should add the columns of this matrix to the original `df`, resulting in a single data frame that contains both the word counts and the sentiments. \n",
    "\n",
    "I constructed my `CountVectorizer` with these settings:\n",
    "\n",
    "```CountVectorizer(max_df = 0.2, min_df = 50, stop_words = 'english')```\n",
    "\n",
    "However, you are free to experiment with different ones if you'd like to. You may find it useful to consult code from the [lecture on the term-document matrix](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/NLP/NLP_1.ipynb) or the [lecture on sentiment analysis](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/NLP/NLP_3.ipynb) for this part.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §3.Train-Test Split\n",
    "\n",
    "Split the data into training and test sets. Use 50% of the data for training and 50% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §4.Prepare Predictor and Target Variables\n",
    "\n",
    "So far, we've split our data into training and test sets, but we also need to separate out the predictor and target variables. We should also make sure that both sets of variables are encoded as numeric columns that a machine learning algorithm can understand. \n",
    "\n",
    "Write a function called `prepare_variables` to perform these tasks. It should have two return values: \n",
    "\n",
    "- `X`, the predictor columns. I suggest dropping all columns except the ones that you constructed as part of the term-document matrix. You can do this using `df.drop(list_of_columns, axis = 1)`. \n",
    "- `y`, the target column. There are multiple valid ways to construct `y` from the original `Sentiment` column of the data. For this activity, `y[i]` should be equal to `1` if `df[\"Sentiment\"][i]` contains the word `\"Positive\"` and `0` otherwise. \n",
    "    - ***Hint***: `df[\"column\"].str.contains(\"word\")`\n",
    "\n",
    "After you've defined your function, use it to prepare the training and test sets:  \n",
    "\n",
    "```\n",
    "X_train, y_train = prepare_variables(train) \n",
    "X_test,  y_test  = prepare_variables(test)\n",
    "```\n",
    "\n",
    "Check these sets to ensure that your function does what you'd expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §5. Fit the model\n",
    "\n",
    "Fit a logistic regression model to the training data. It is possible to perform cross-validation to set model complexity parameters, but we won't worry about that today. \n",
    "\n",
    "You may receive a warning indicating that \"TOTAL NO. of ITERATIONS REACHED LIMIT.\" This is a little bit intimidating, but in this case it's not really harmful. It's fine to ignore this warning, or you can set the parameter `max_iter` when you construct the `LogisticRegression()` model to a higher value, such as `500` or `1000`. This may increase the amount of time required to fit your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, evaluate the accuracy of your model on the training and testing data. You may observe a small amount of overfitting, indicated by the testing score being lower than the training score. Provided that both are over 80%, you're doing fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §6. Positive and Negative Words\n",
    "\n",
    "The *coefficients* associated by logistic regression to each word are stored in the `LR.coef_[0]` attribute. Show lists of the most positive and most negative words. Discuss your findings -- do these words look as though they would reasonably convey positive or negative sentiment?\n",
    "\n",
    "***Hint***: *There are several good ways to do this. I found that the easiest way was to create a new `Dataframe` containing `LR.coef_[0]` in one column and `X_train.columns` (the list of words) in the other column. I then used `df.sort_values()` to sort on the column containing coefficients. The [lecture on sentiment analysis](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/NLP/NLP_3.ipynb) illustrates this method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### §7. Inspect Errors\n",
    "\n",
    "Finally, we should take a look at a few examples in which the model made a mistake on the test data. Inspect three tweets in which the model's prediction and the actual value differ. Comment on why you think the error may have occurred. \n",
    "\n",
    "**Don't overthink it!** It's sufficient to simply identify three rows in which the prediction disagrees with the true label. You shouldn't need more than 3-4 lines. \n",
    "\n",
    "***Note***: *One valid response is to disagree with the sentiment label assigned to the tweet -- some of them are a little odd.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
